{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11444158,"sourceType":"datasetVersion","datasetId":7100347}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset\nimport torch\nimport numpy as np\n\n\nclass CC200Data(Dataset):\n    def __init__(self, path, mapping, labels):\n        super().__init__()\n        self.path = path\n        self.mapping = mapping\n        self.folder = os.listdir(self.path)\n        self.labels = self._map_labels(labels)\n        self.files = [self._fix_nans(np.loadtxt(f\"{path}/{i}\")) for i in self.folder]\n        self.region_indices = self._region_mapping()\n        self.region_coeff = torch.tensor([self._region_coeffs(i) for i in self.files], dtype=torch.float32)\n        self.subnetwork_coeff = [self._subnetwork_coeffs(i) for i in self.files]\n        self.region_start_indices = {}\n        for i in range(1, 8):\n            if i == 1:\n                self.region_start_indices[i] = 0\n            else:\n                self.region_start_indices[i] = self.region_start_indices[i - 1] + len(\n                    self.region_indices[i - 1]\n                )\n                \n    def _map_labels(self, mapping):\n        labels = []\n        for i in self.folder:\n            if i[:-14] in mapping.keys():\n                labels.append(mapping[i[:-14]])\n            else:\n                print(i)\n\n        return torch.tensor(labels, dtype=torch.long)\n        \n    def _fix_nans(self, x):\n        std = np.std(x, axis=0)\n        zeroes = std == 0\n    \n        if zeroes.any():\n            noise = np.random.normal(loc=0.0, scale=1e-6, size=(x.shape[0], zeroes.sum()))\n            x[:, zeroes] = noise\n    \n        return x\n\n    def _region_coeffs(self, x):\n        b = np.zeros_like(x)\n        y = 0\n        for i in self.region_indices:\n            b[:, y : y + len(self.region_indices[i])] = x[:, self.region_indices[i]]\n            y += len(self.region_indices[i])\n        b = b[:, 15:]\n        x = np.corrcoef(b, rowvar=False)\n        return x\n\n    def _region_mapping(self):\n        region_indices = {i: [] for i in range(0, 8)}\n        for i, j in self.mapping.items():\n            region_indices[j].append(i - 1)\n        return region_indices\n\n    def _subnetwork_coeffs(self, x):\n        correlation_matrices = []\n\n        for file_idx, (region_id, indices) in enumerate(self.region_indices.items()):\n            if not indices:\n                print(f\"[INFO] Region {region_id} has no indices, skipping.\")\n                continue\n\n            submatrix = x[:, indices]\n            std = np.std(submatrix, axis=0)\n\n            zero_std_count = np.sum(std == 0)\n            if zero_std_count > 0:\n                print(f\"[INFO] File {self.folder[file_idx]} - Region {region_id} has {zero_std_count} constant columns.\")\n\n            try:\n                correlation_matrix = np.corrcoef(submatrix, rowvar=False)\n            except Exception as e:\n                print(f\"[ERROR] Correlation failed in region {region_id}\")\n                print(f\"Exception: {e}\")\n                continue\n\n            flat_corr = self._flatten_matrix(correlation_matrix)\n            correlation_matrices.append(torch.tensor(flat_corr, dtype=torch.float32))\n\n        return correlation_matrices\n\n    def _flatten_matrix(self, matrix):\n        idx = np.triu_indices_from(matrix, k=1)\n        return matrix[idx]\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        region_corr = self.region_coeff[idx]\n        subnetwork_corrs = self.subnetwork_coeff[idx]\n        labels = self.labels[idx]\n        return region_corr, subnetwork_corrs, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:08:04.733817Z","iopub.execute_input":"2025-04-22T03:08:04.734017Z","iopub.status.idle":"2025-04-22T03:08:11.855746Z","shell.execute_reply.started":"2025-04-22T03:08:04.733997Z","shell.execute_reply":"2025-04-22T03:08:11.854919Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/broccubali/AutisticAdventures/main/cc200_to_yeo7_mapping.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:08:48.778033Z","iopub.execute_input":"2025-04-22T03:08:48.778873Z","iopub.status.idle":"2025-04-22T03:08:49.079828Z","shell.execute_reply.started":"2025-04-22T03:08:48.778845Z","shell.execute_reply":"2025-04-22T03:08:49.079119Z"}},"outputs":[{"name":"stdout","text":"--2025-04-22 03:08:48--  https://raw.githubusercontent.com/broccubali/AutisticAdventures/main/cc200_to_yeo7_mapping.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1319 (1.3K) [text/plain]\nSaving to: ‘cc200_to_yeo7_mapping.csv’\n\ncc200_to_yeo7_mappi 100%[===================>]   1.29K  --.-KB/s    in 0s      \n\n2025-04-22 03:08:48 (86.5 MB/s) - ‘cc200_to_yeo7_mapping.csv’ saved [1319/1319]\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"! wget https://s3.amazonaws.com/fcp-indi/data/Projects/ABIDE_Initiative/Phenotypic_V1_0b_preprocessed1.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:08:43.370593Z","iopub.execute_input":"2025-04-22T03:08:43.371275Z","iopub.status.idle":"2025-04-22T03:08:43.882739Z","shell.execute_reply.started":"2025-04-22T03:08:43.371248Z","shell.execute_reply":"2025-04-22T03:08:43.882083Z"}},"outputs":[{"name":"stdout","text":"--2025-04-22 03:08:43--  https://s3.amazonaws.com/fcp-indi/data/Projects/ABIDE_Initiative/Phenotypic_V1_0b_preprocessed1.csv\nResolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.184.213, 16.182.72.40, 52.216.107.174, ...\nConnecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.184.213|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 449443 (439K) [application/octet-stream]\nSaving to: ‘Phenotypic_V1_0b_preprocessed1.csv’\n\nPhenotypic_V1_0b_pr 100%[===================>] 438.91K  --.-KB/s    in 0.1s    \n\n2025-04-22 03:08:43 (4.02 MB/s) - ‘Phenotypic_V1_0b_preprocessed1.csv’ saved [449443/449443]\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('cc200_to_yeo7_mapping.csv')\ncc200_to_yeo7_mapping = dict(zip(df['CC200_Region'], df['Yeo7_Network'])) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:08:51.762946Z","iopub.execute_input":"2025-04-22T03:08:51.763726Z","iopub.status.idle":"2025-04-22T03:08:51.790388Z","shell.execute_reply.started":"2025-04-22T03:08:51.763691Z","shell.execute_reply":"2025-04-22T03:08:51.789671Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df = pd.read_csv(\"Phenotypic_V1_0b_preprocessed1.csv\")\ndf = df[[\"FILE_ID\", \"DX_GROUP\"]]\nlabels_mapping = dict(zip(df[\"FILE_ID\"], df[\"DX_GROUP\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:09:05.934007Z","iopub.execute_input":"2025-04-22T03:09:05.934711Z","iopub.status.idle":"2025-04-22T03:09:05.971180Z","shell.execute_reply.started":"2025-04-22T03:09:05.934681Z","shell.execute_reply":"2025-04-22T03:09:05.970689Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# had to fix this here cuz pytorch cross entropy loss needs 0 and 1 not 1 and 2\nnew_labels_mapping = {}\nfor key, value in labels_mapping.items():\n    new_labels_mapping[key] = value - 1  # Subtract 1 to convert 1,2 to 0,1\n\n# Recreate your dataset with adjusted labels\ndataset = CC200Data(\"/kaggle/input/autistic-brains/Outputs/cpac/nofilt_noglobal/rois_cc200\", cc200_to_yeo7_mapping, new_labels_mapping)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:09:41.903181Z","iopub.execute_input":"2025-04-22T03:09:41.903902Z","iopub.status.idle":"2025-04-22T03:10:04.610808Z","shell.execute_reply.started":"2025-04-22T03:09:41.903876Z","shell.execute_reply":"2025-04-22T03:10:04.610156Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2297020341.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  self.region_coeff = torch.tensor([self._region_coeffs(i) for i in self.files], dtype=torch.float32)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ntrain_loader = DataLoader(dataset, batch_size=16, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:10:04.611970Z","iopub.execute_input":"2025-04-22T03:10:04.612243Z","iopub.status.idle":"2025-04-22T03:10:04.616281Z","shell.execute_reply.started":"2025-04-22T03:10:04.612214Z","shell.execute_reply":"2025-04-22T03:10:04.615705Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"unique_labels = set()\nfor _, _, labels in train_loader:\n    unique_labels.update(labels.numpy())\nprint(f\"updated label values: {sorted(list(unique_labels))}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:10:04.616826Z","iopub.execute_input":"2025-04-22T03:10:04.616993Z","iopub.status.idle":"2025-04-22T03:10:04.763949Z","shell.execute_reply.started":"2025-04-22T03:10:04.616979Z","shell.execute_reply":"2025-04-22T03:10:04.763201Z"}},"outputs":[{"name":"stdout","text":"updated label values: [0, 1]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"shapes = []\na = next(iter(dataset))[1]\nfor i in a:\n    shapes.append(i.shape[0])\n\n# len(shapes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:10:08.330087Z","iopub.execute_input":"2025-04-22T03:10:08.330347Z","iopub.status.idle":"2025-04-22T03:10:08.334251Z","shell.execute_reply.started":"2025-04-22T03:10:08.330326Z","shell.execute_reply":"2025-04-22T03:10:08.333693Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MHSA(nn.Module):\n    def __init__(self, embd_dim, num_heads):\n        super().__init__()\n        self.embd_dim = embd_dim\n        self.num_heads = num_heads\n        self.head_size = self.embd_dim // self.num_heads\n        self.q = nn.Linear(self.embd_dim, self.embd_dim)\n        self.k = nn.Linear(self.embd_dim, self.embd_dim)\n        self.v = nn.Linear(self.embd_dim, self.embd_dim)\n        self.d = self.head_size ** 0.5\n        self.mlp = nn.Linear(self.embd_dim, self.embd_dim)\n        self.layer_norm = nn.LayerNorm(self.embd_dim)  \n        \n    def forward(self, x):\n        batch_size, M, _ = x.shape\n        norm = self.layer_norm(x)\n        q = self.q(norm).view(batch_size, M, self.num_heads, self.head_size).transpose(1, 2)\n        k = self.k(norm).view(batch_size, M, self.num_heads, self.head_size).transpose(1, 2)\n        v = self.v(norm).view(batch_size, M, self.num_heads, self.head_size).transpose(1, 2)\n        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / self.d\n        attn_scores = attn_scores.masked_fill(torch.eye(M, device=x.device).bool(), float('-inf'))\n        attn_weights = F.softmax(attn_scores, dim=-1)\n        context = torch.matmul(attn_weights, v).transpose(1, 2).reshape(batch_size, M, self.embd_dim)\n        out = self.mlp(context)\n        return out + x, attn_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:10:18.004380Z","iopub.execute_input":"2025-04-22T03:10:18.004962Z","iopub.status.idle":"2025-04-22T03:10:18.012023Z","shell.execute_reply.started":"2025-04-22T03:10:18.004935Z","shell.execute_reply":"2025-04-22T03:10:18.011412Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class SubnetworkEmbedder(nn.Module):\n    def __init__(self, input_dim, hidden_dim=256, output_dim=128):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim),\n        )\n\n    def forward(self, x):\n        return self.mlp(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:10:25.878954Z","iopub.execute_input":"2025-04-22T03:10:25.879483Z","iopub.status.idle":"2025-04-22T03:10:25.883524Z","shell.execute_reply.started":"2025-04-22T03:10:25.879453Z","shell.execute_reply":"2025-04-22T03:10:25.882954Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class RegionEmbedder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.region_conv = nn.Conv2d(1, 1, kernel_size=1)\n        self.mlp = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim),\n        )       \n\n    def forward(self, x):\n        x_conv = self.region_conv(x.unsqueeze(1)) \n        x_conv = x_conv.squeeze(1)  \n        return self.mlp(x_conv) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:10:33.715839Z","iopub.execute_input":"2025-04-22T03:10:33.716116Z","iopub.status.idle":"2025-04-22T03:10:33.721021Z","shell.execute_reply.started":"2025-04-22T03:10:33.716089Z","shell.execute_reply":"2025-04-22T03:10:33.720267Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class RegionEncoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, embd_dim, num_heads, num_layers):\n        super().__init__()\n        self.reg_embd = RegionEmbedder(input_dim, hidden_dim, embd_dim)\n        self.mhsa_layers = nn.ModuleList([MHSA(embd_dim, num_heads) for _ in range(num_layers)])\n\n    def forward(self, x):\n        x_reg = self.reg_embd(x)\n        x_in = x_reg\n        attn_weights_all = []\n        for mhsa in self.mhsa_layers:\n            x_in, attn_weights = mhsa(x_in)\n            attn_weights_all.append(attn_weights)\n        \n        return x_reg + x_in, torch.stack(attn_weights_all).permute(1, 0, 2, 3, 4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:10:43.504490Z","iopub.execute_input":"2025-04-22T03:10:43.504770Z","iopub.status.idle":"2025-04-22T03:10:43.510009Z","shell.execute_reply.started":"2025-04-22T03:10:43.504747Z","shell.execute_reply":"2025-04-22T03:10:43.509433Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class SubNetworkEncoder(nn.Module):\n    def __init__(self, shapes, hidden_dim, embd_dim, num_heads, num_layers):\n        super().__init__()\n        self.embd_dim = embd_dim\n        self.mlps = nn.ModuleList([SubnetworkEmbedder(i, hidden_dim, embd_dim) for i in shapes])\n        self.mhsa_layers = nn.ModuleList([MHSA(embd_dim, num_heads) for _ in range(num_layers)])\n\n    def forward(self, x):\n        batch_size = x[0].shape[0]\n        x = torch.stack([mlp(f) for mlp, f in zip(self.mlps, x)], dim=1)\n        attn_weights_all = []\n        for mhsa in self.mhsa_layers:\n            x, attn_weights = mhsa(x)\n            attn_weights_all.append(attn_weights)\n\n        return x, torch.stack(attn_weights_all).permute(1, 0, 2, 3, 4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:10:55.778285Z","iopub.execute_input":"2025-04-22T03:10:55.778839Z","iopub.status.idle":"2025-04-22T03:10:55.784090Z","shell.execute_reply.started":"2025-04-22T03:10:55.778816Z","shell.execute_reply":"2025-04-22T03:10:55.783517Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class StepOne(nn.Module):\n    def __init__(self, input_dim, hidden_dim, embd_dim, num_heads, num_layers):\n        super().__init__()\n        self.reg_enc = RegionEncoder(input_dim, hidden_dim, embd_dim, num_heads, num_layers)\n        self.subnet_enc = SubNetworkEncoder(shapes, hidden_dim, embd_dim, num_heads, num_layers)\n        self.layer_norm = nn.LayerNorm(embd_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(embd_dim, hidden_dim),  \n            nn.ReLU(),             \n            nn.Linear(hidden_dim, embd_dim)    \n        )\n        self.region_start_indices = list(dataset.region_start_indices.values()) + [185]\n\n    def subNetworkAttendRegions(self, subnet_attn_map, region_attn_map):\n        region_to_subnet = torch.zeros(185, dtype=torch.long)\n        for subnet_id in range(7):\n            start = self.region_start_indices[subnet_id]\n            end = self.region_start_indices[subnet_id + 1]\n            region_to_subnet[start:end] = subnet_id  \n        subnet_i = region_to_subnet.view(-1, 1).expand(185, 185) \n        subnet_j = region_to_subnet.view(1, -1).expand(185, 185)  \n\n        mask = subnet_i != subnet_j  \n\n        attn_multiplier = subnet_attn_map[:, :, :, subnet_i, subnet_j]  \n        attn_multiplier = attn_multiplier * mask\n\n        return region_attn_map * attn_multiplier \n    \n    def sinkhorn(self, attn, n_iters=5, eps=1e-6):\n        attn = attn + eps  \n        for _ in range(n_iters):\n            attn = attn / attn.sum(dim=-1, keepdim=True)\n            attn = attn / attn.sum(dim=-2, keepdim=True)\n        return attn\n    \n    def forward(self, x):\n        x0 = self.reg_enc(x[0])\n        x1 = self.subnet_enc(x[1])\n        o = torch.cat((x0[0], x1[0]), dim=1)\n        o_norm = self.layer_norm(o)\n        o_norm = self.mlp(o)\n        o = o + o_norm\n        print(o.shape)\n        o_reg = o[:, :185, :]\n        o_sub = o[:, 185:, :]\n        adj_matrix = self.subNetworkAttendRegions(x1[1], x0[1])\n        adj_matrix = self.sinkhorn(adj_matrix)\n        return o_reg, o_sub, adj_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:14:11.927381Z","iopub.execute_input":"2025-04-22T03:14:11.928142Z","iopub.status.idle":"2025-04-22T03:14:11.936467Z","shell.execute_reply.started":"2025-04-22T03:14:11.928114Z","shell.execute_reply":"2025-04-22T03:14:11.935816Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class HGCN(nn.Module):\n    def __init__(self, input_dim, output_dim, num_layers=4, num_heads=16):\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n\n        # One linear layer per head per layer: (L, H, in, out)\n        self.W = nn.Parameter(torch.randn(num_layers, num_heads, input_dim, output_dim))\n        self.activation = nn.ReLU()\n\n    def forward(self, features, attention_maps):\n        \"\"\"\n        features: [B, N, F_in]\n        attention_maps: [B, L, H, N, N] — soft adjacency or incidence maps\n        \"\"\"\n        B, L, H, N, _ = attention_maps.shape\n        F_in, F_out = self.input_dim, self.output_dim\n\n        # Apply W to input features: [B, 1, 1, N, F_in] x [L, H, F_in, F_out]\n        # -> output: [B, L, H, N, F_out]\n        features_exp = features[:, None, None, :, :]  # [B, 1, 1, N, F_in]\n        weights = self.W[None, :, :, :, :]            # [1, L, H, F_in, F_out]\n        transformed = torch.matmul(features_exp, weights)  # [B, L, H, N, F_out]\n\n        # Apply hypergraph attention maps\n        # attention_maps: [B, L, H, N, N]\n        # transformed:     [B, L, H, N, F_out]\n        output = torch.matmul(attention_maps, transformed)  # [B, L, H, N, F_out]\n        output = self.activation(output)\n\n        # Concatenate heads → [B, L, N, H * F_out]\n        output = output.permute(0, 1, 3, 2, 4).reshape(B, L, N, H * F_out)\n\n        # Concatenate layers → [B, N, L * H * F_out]\n        output = output.permute(0, 2, 1, 3).reshape(B, N, L * H * F_out)\n\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:14:26.085419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class StepOneWithHGCN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, embd_dim, num_heads, num_layers, num_classes=2):\n        super().__init__()\n        # Copy paste\n        self.reg_enc = RegionEncoder(input_dim, hidden_dim, embd_dim, num_heads, num_layers)\n        self.subnet_enc = SubNetworkEncoder(shapes, hidden_dim, embd_dim, num_heads, num_layers)\n        self.layer_norm = nn.LayerNorm(embd_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(embd_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, embd_dim)\n        )\n        self.region_start_indices = list(dataset.region_start_indices.values()) + [185]\n        self.hgcn = HGCN(input_dim=embd_dim, output_dim=embd_dim//num_heads)\n        hgcn_output_dim = (embd_dim//num_heads) * num_heads * num_layers\n        self.classifier = nn.Sequential(\n            nn.Linear(hgcn_output_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_dim, num_classes)\n        )\n    \n    def subNetworkAttendRegions(self, subnet_attn_map, region_attn_map):\n        region_to_subnet = torch.zeros(185, dtype=torch.long)\n        for subnet_id in range(7):\n            start = self.region_start_indices[subnet_id]\n            end = self.region_start_indices[subnet_id + 1]\n            region_to_subnet[start:end] = subnet_id\n        subnet_i = region_to_subnet.view(-1, 1).expand(185, 185)\n        subnet_j = region_to_subnet.view(1, -1).expand(185, 185)\n        mask = subnet_i != subnet_j\n        attn_multiplier = subnet_attn_map[:, :, :, subnet_i, subnet_j]\n        mask = mask.to(\"cuda\")\n        attn_multiplier = attn_multiplier * mask\n        return region_attn_map * attn_multiplier\n    \n    def sinkhorn(self, attn, n_iters=5, eps=1e-6):\n        attn = attn + eps\n        for _ in range(n_iters):\n            attn = attn / attn.sum(dim=-1, keepdim=True)\n            attn = attn / attn.sum(dim=-2, keepdim=True)\n        return attn\n    \n    def forward(self, x):\n        x0, region_attn = self.reg_enc(x[0])\n        x1, subnet_attn = self.subnet_enc(x[1])\n\n        o = torch.cat((x0, x1), dim=1)\n        o_norm = self.layer_norm(o)\n        o_norm = self.mlp(o_norm)\n        o = o + o_norm\n        o_reg = o[:, :185, :]  # First 185 nodes are regions\n        o_sub = o[:, 185:, :]  # Remaining nodes are subnetworks\n        # Process attention maps to create the combined attention \n        # with shape [batch_size, num_layers (4), num_heads (16), 185, 185]\n        # combined_attn = self.subNetworkAttendRegions(subnet_attn, region_attn)\n        # combined_attn = self.sinkhorn(combined_attn)\n        \n        # # Pass through HGCN - only process the region features with the combined attention\n        # hgcn_output = self.hgcn(o_reg, combined_attn)\n\n        # Option 1: combined attention\n        # combined_attn = self.subNetworkAttendRegions(subnet_attn, region_attn)\n        # combined_attn = self.sinkhorn(combined_attn)\n        \n        # Option 2: sub-net attention\n        region_attn = self.sinkhorn(region_attn)\n        combined_attn = region_attn\n\n        hgcn_output = self.hgcn(o_reg, combined_attn) # leave unchanged\n\n        # Global average pooling for classification\n        pooled_output = hgcn_output.mean(dim=1)  # [batch_size, output_dim*num_heads*num_layers]\n        # Final classif\n        logits = self.classifier(pooled_output)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:17:50.342685Z","iopub.execute_input":"2025-04-22T03:17:50.342930Z","iopub.status.idle":"2025-04-22T03:17:50.354161Z","shell.execute_reply.started":"2025-04-22T03:17:50.342915Z","shell.execute_reply":"2025-04-22T03:17:50.353555Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"from tqdm import tqdm\n\nmodel = StepOneWithHGCN(\n    input_dim=185,\n    hidden_dim=256,\n    embd_dim=128,\n    num_heads=16,\n    num_layers=4,\n    num_classes=2\n)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\ncriterion = nn.CrossEntropyLoss()\n\nnum_epochs = 50\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    # tqdm for batch progress\n    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n    \n    for batch_idx, (region_data, subnetwork_data, labels) in pbar:\n        region_data = region_data.to(device)\n        subnetwork_data = [subnet.to(device) for subnet in subnetwork_data]\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n\n        x = (region_data, subnetwork_data)\n        logits = model(x)\n        loss = criterion(logits, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(logits.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n        pbar.set_postfix({\n            'Loss': f'{running_loss / (batch_idx + 1):.4f}',\n            'Acc': f'{100 * correct / total:.2f}%'\n        })\n\ntorch.save(model.state_dict(), 'brain_network_model.pth')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T03:17:52.160922Z","iopub.execute_input":"2025-04-22T03:17:52.161184Z","iopub.status.idle":"2025-04-22T03:24:28.318449Z","shell.execute_reply.started":"2025-04-22T03:17:52.161163Z","shell.execute_reply":"2025-04-22T03:24:28.317924Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/50: 100%|██████████| 56/56 [00:08<00:00,  6.53it/s, Loss=0.7821, Acc=50.57%]\nEpoch 2/50: 100%|██████████| 56/56 [00:07<00:00,  7.24it/s, Loss=0.7567, Acc=52.38%]\nEpoch 3/50: 100%|██████████| 56/56 [00:07<00:00,  7.25it/s, Loss=0.7362, Acc=51.36%]\nEpoch 4/50: 100%|██████████| 56/56 [00:07<00:00,  7.25it/s, Loss=0.7141, Acc=54.52%]\nEpoch 5/50: 100%|██████████| 56/56 [00:07<00:00,  7.23it/s, Loss=0.7184, Acc=50.68%]\nEpoch 6/50: 100%|██████████| 56/56 [00:07<00:00,  7.23it/s, Loss=0.7250, Acc=50.11%]\nEpoch 7/50: 100%|██████████| 56/56 [00:07<00:00,  7.22it/s, Loss=0.7178, Acc=54.07%]\nEpoch 8/50: 100%|██████████| 56/56 [00:07<00:00,  7.21it/s, Loss=0.7127, Acc=52.04%]\nEpoch 9/50: 100%|██████████| 56/56 [00:07<00:00,  7.20it/s, Loss=0.7093, Acc=51.70%]\nEpoch 10/50: 100%|██████████| 56/56 [00:07<00:00,  7.17it/s, Loss=0.6998, Acc=51.58%]\nEpoch 11/50: 100%|██████████| 56/56 [00:07<00:00,  7.16it/s, Loss=0.7011, Acc=48.42%]\nEpoch 12/50: 100%|██████████| 56/56 [00:07<00:00,  7.15it/s, Loss=0.6935, Acc=54.30%]\nEpoch 13/50: 100%|██████████| 56/56 [00:07<00:00,  7.12it/s, Loss=0.6941, Acc=52.83%]\nEpoch 14/50: 100%|██████████| 56/56 [00:07<00:00,  7.08it/s, Loss=0.6971, Acc=52.71%]\nEpoch 15/50: 100%|██████████| 56/56 [00:07<00:00,  7.06it/s, Loss=0.6989, Acc=54.30%]\nEpoch 16/50: 100%|██████████| 56/56 [00:07<00:00,  7.00it/s, Loss=0.7189, Acc=51.36%]\nEpoch 17/50: 100%|██████████| 56/56 [00:08<00:00,  6.92it/s, Loss=0.7339, Acc=50.34%]\nEpoch 18/50: 100%|██████████| 56/56 [00:08<00:00,  6.90it/s, Loss=0.7018, Acc=50.79%]\nEpoch 19/50: 100%|██████████| 56/56 [00:08<00:00,  6.99it/s, Loss=0.6944, Acc=53.17%]\nEpoch 20/50: 100%|██████████| 56/56 [00:07<00:00,  7.05it/s, Loss=0.6967, Acc=51.02%]\nEpoch 21/50: 100%|██████████| 56/56 [00:07<00:00,  7.07it/s, Loss=0.6915, Acc=54.19%]\nEpoch 22/50: 100%|██████████| 56/56 [00:07<00:00,  7.08it/s, Loss=0.6924, Acc=53.28%]\nEpoch 23/50: 100%|██████████| 56/56 [00:07<00:00,  7.08it/s, Loss=0.6930, Acc=52.60%]\nEpoch 24/50: 100%|██████████| 56/56 [00:07<00:00,  7.07it/s, Loss=0.6942, Acc=52.15%]\nEpoch 25/50: 100%|██████████| 56/56 [00:07<00:00,  7.07it/s, Loss=0.6923, Acc=54.52%]\nEpoch 26/50: 100%|██████████| 56/56 [00:07<00:00,  7.04it/s, Loss=0.6930, Acc=53.51%]\nEpoch 27/50: 100%|██████████| 56/56 [00:07<00:00,  7.04it/s, Loss=0.6918, Acc=53.39%]\nEpoch 28/50: 100%|██████████| 56/56 [00:07<00:00,  7.03it/s, Loss=0.6937, Acc=52.71%]\nEpoch 29/50: 100%|██████████| 56/56 [00:07<00:00,  7.05it/s, Loss=0.6949, Acc=53.05%]\nEpoch 30/50: 100%|██████████| 56/56 [00:07<00:00,  7.05it/s, Loss=0.6922, Acc=54.07%]\nEpoch 31/50: 100%|██████████| 56/56 [00:07<00:00,  7.06it/s, Loss=0.6916, Acc=55.32%]\nEpoch 32/50: 100%|██████████| 56/56 [00:07<00:00,  7.06it/s, Loss=0.6892, Acc=54.98%]\nEpoch 33/50: 100%|██████████| 56/56 [00:07<00:00,  7.06it/s, Loss=0.6904, Acc=53.39%]\nEpoch 34/50: 100%|██████████| 56/56 [00:07<00:00,  7.06it/s, Loss=0.6961, Acc=52.26%]\nEpoch 35/50: 100%|██████████| 56/56 [00:07<00:00,  7.07it/s, Loss=0.6921, Acc=53.28%]\nEpoch 36/50: 100%|██████████| 56/56 [00:07<00:00,  7.07it/s, Loss=0.6971, Acc=50.90%]\nEpoch 37/50: 100%|██████████| 56/56 [00:07<00:00,  7.07it/s, Loss=0.6949, Acc=53.05%]\nEpoch 38/50: 100%|██████████| 56/56 [00:07<00:00,  7.06it/s, Loss=0.6905, Acc=54.30%]\nEpoch 39/50: 100%|██████████| 56/56 [00:07<00:00,  7.06it/s, Loss=0.6940, Acc=52.26%]\nEpoch 40/50: 100%|██████████| 56/56 [00:07<00:00,  7.06it/s, Loss=0.6906, Acc=54.07%]\nEpoch 41/50: 100%|██████████| 56/56 [00:07<00:00,  7.07it/s, Loss=0.6940, Acc=50.79%]\nEpoch 42/50: 100%|██████████| 56/56 [00:07<00:00,  7.05it/s, Loss=0.6934, Acc=52.49%]\nEpoch 43/50: 100%|██████████| 56/56 [00:07<00:00,  7.06it/s, Loss=0.6901, Acc=53.51%]\nEpoch 44/50: 100%|██████████| 56/56 [00:07<00:00,  7.06it/s, Loss=0.6916, Acc=53.85%]\nEpoch 45/50: 100%|██████████| 56/56 [00:07<00:00,  7.04it/s, Loss=0.6933, Acc=52.83%]\nEpoch 46/50: 100%|██████████| 56/56 [00:07<00:00,  7.05it/s, Loss=0.6908, Acc=54.64%]\nEpoch 47/50: 100%|██████████| 56/56 [00:07<00:00,  7.04it/s, Loss=0.6903, Acc=54.30%]\nEpoch 48/50: 100%|██████████| 56/56 [00:07<00:00,  7.05it/s, Loss=0.6911, Acc=53.39%]\nEpoch 49/50: 100%|██████████| 56/56 [00:07<00:00,  7.05it/s, Loss=0.6896, Acc=53.85%]\nEpoch 50/50: 100%|██████████| 56/56 [00:07<00:00,  7.04it/s, Loss=0.6922, Acc=54.19%]\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}