{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11444158,"sourceType":"datasetVersion","datasetId":7100347}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset\nimport torch\nimport numpy as np\n\n\nclass CC200Data(Dataset):\n    def __init__(self, path, mapping, labels):\n        super().__init__()\n        self.path = path\n        self.mapping = mapping\n        self.folder = os.listdir(self.path)\n        self.labels = self._map_labels(labels)\n        self.files = [self._fix_nans(np.loadtxt(f\"{path}/{i}\")) for i in self.folder]\n        self.region_indices = self._region_mapping()\n        self.region_coeff = torch.tensor([self._region_coeffs(i) for i in self.files], dtype=torch.float32)\n        self.subnetwork_coeff = [self._subnetwork_coeffs(i) for i in self.files]\n        self.region_start_indices = {}\n        for i in range(1, 8):\n            if i == 1:\n                self.region_start_indices[i] = 0\n            else:\n                self.region_start_indices[i] = self.region_start_indices[i - 1] + len(\n                    self.region_indices[i - 1]\n                )\n                \n    def _map_labels(self, mapping):\n        labels = []\n        for i in self.folder:\n            if i[:-14] in mapping.keys():\n                labels.append(mapping[i[:-14]])\n            else:\n                print(i)\n\n        return torch.tensor(labels, dtype=torch.long)\n        \n    def _fix_nans(self, x):\n        std = np.std(x, axis=0)\n        zeroes = std == 0\n    \n        if zeroes.any():\n            noise = np.random.normal(loc=0.0, scale=1e-6, size=(x.shape[0], zeroes.sum()))\n            x[:, zeroes] = noise\n    \n        return x\n\n    def _region_coeffs(self, x):\n        b = np.zeros_like(x)\n        y = 0\n        for i in self.region_indices:\n            b[:, y : y + len(self.region_indices[i])] = x[:, self.region_indices[i]]\n            y += len(self.region_indices[i])\n        b = b[:, 15:]\n        x = np.corrcoef(b, rowvar=False)\n        return x\n\n    def _region_mapping(self):\n        region_indices = {i: [] for i in range(0, 8)}\n        for i, j in self.mapping.items():\n            region_indices[j].append(i - 1)\n        return region_indices\n\n    def _subnetwork_coeffs(self, x):\n        correlation_matrices = []\n\n        for file_idx, (region_id, indices) in enumerate(self.region_indices.items()):\n            if not indices:\n                print(f\"[INFO] Region {region_id} has no indices, skipping.\")\n                continue\n\n            submatrix = x[:, indices]\n            std = np.std(submatrix, axis=0)\n\n            zero_std_count = np.sum(std == 0)\n            if zero_std_count > 0:\n                print(f\"[INFO] File {self.folder[file_idx]} - Region {region_id} has {zero_std_count} constant columns.\")\n\n            try:\n                correlation_matrix = np.corrcoef(submatrix, rowvar=False)\n            except Exception as e:\n                print(f\"[ERROR] Correlation failed in region {region_id}\")\n                print(f\"Exception: {e}\")\n                continue\n\n            flat_corr = self._flatten_matrix(correlation_matrix)\n            correlation_matrices.append(torch.tensor(flat_corr, dtype=torch.float32))\n\n        return correlation_matrices\n\n    def _flatten_matrix(self, matrix):\n        idx = np.triu_indices_from(matrix, k=1)\n        return matrix[idx]\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        region_corr = self.region_coeff[idx]\n        subnetwork_corrs = self.subnetwork_coeff[idx]\n        labels = self.labels[idx]\n        return region_corr, subnetwork_corrs, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:08:16.206121Z","iopub.execute_input":"2025-04-18T16:08:16.206833Z","iopub.status.idle":"2025-04-18T16:08:23.036411Z","shell.execute_reply.started":"2025-04-18T16:08:16.206788Z","shell.execute_reply":"2025-04-18T16:08:23.035831Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/broccubali/AutisticAdventures/main/cc200_to_yeo7_mapping.csv\n!wget https://s3.amazonaws.com/fcp-indi/data/Projects/ABIDE_Initiative/Phenotypic_V1_0b_preprocessed1.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:08:23.037755Z","iopub.execute_input":"2025-04-18T16:08:23.038115Z","iopub.status.idle":"2025-04-18T16:08:23.739848Z","shell.execute_reply.started":"2025-04-18T16:08:23.038090Z","shell.execute_reply":"2025-04-18T16:08:23.739055Z"}},"outputs":[{"name":"stdout","text":"--2025-04-18 16:08:23--  https://raw.githubusercontent.com/broccubali/AutisticAdventures/main/cc200_to_yeo7_mapping.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1319 (1.3K) [text/plain]\nSaving to: ‘cc200_to_yeo7_mapping.csv’\n\ncc200_to_yeo7_mappi 100%[===================>]   1.29K  --.-KB/s    in 0s      \n\n2025-04-18 16:08:23 (57.8 MB/s) - ‘cc200_to_yeo7_mapping.csv’ saved [1319/1319]\n\n--2025-04-18 16:08:23--  https://s3.amazonaws.com/fcp-indi/data/Projects/ABIDE_Initiative/Phenotypic_V1_0b_preprocessed1.csv\nResolving s3.amazonaws.com (s3.amazonaws.com)... 3.5.2.37, 54.231.129.240, 52.217.234.0, ...\nConnecting to s3.amazonaws.com (s3.amazonaws.com)|3.5.2.37|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 449443 (439K) [application/octet-stream]\nSaving to: ‘Phenotypic_V1_0b_preprocessed1.csv’\n\nPhenotypic_V1_0b_pr 100%[===================>] 438.91K  --.-KB/s    in 0.1s    \n\n2025-04-18 16:08:23 (3.92 MB/s) - ‘Phenotypic_V1_0b_preprocessed1.csv’ saved [449443/449443]\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('cc200_to_yeo7_mapping.csv')\ncc200_to_yeo7_mapping = dict(zip(df['CC200_Region'], df['Yeo7_Network'])) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:08:23.740824Z","iopub.execute_input":"2025-04-18T16:08:23.741120Z","iopub.status.idle":"2025-04-18T16:08:24.028755Z","shell.execute_reply.started":"2025-04-18T16:08:23.741097Z","shell.execute_reply":"2025-04-18T16:08:24.028151Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df = pd.read_csv(\"Phenotypic_V1_0b_preprocessed1.csv\")\ndf = df[[\"FILE_ID\", \"DX_GROUP\"]]\nlabels_mapping = dict(zip(df[\"FILE_ID\"], df[\"DX_GROUP\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:08:24.030226Z","iopub.execute_input":"2025-04-18T16:08:24.030443Z","iopub.status.idle":"2025-04-18T16:08:24.062193Z","shell.execute_reply.started":"2025-04-18T16:08:24.030426Z","shell.execute_reply":"2025-04-18T16:08:24.061733Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!ls /kaggle/input","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:08:24.062853Z","iopub.execute_input":"2025-04-18T16:08:24.063104Z","iopub.status.idle":"2025-04-18T16:08:24.202273Z","shell.execute_reply.started":"2025-04-18T16:08:24.063085Z","shell.execute_reply":"2025-04-18T16:08:24.201402Z"}},"outputs":[{"name":"stdout","text":"download_abide_preproc.py  Outputs\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# had to fix this here cuz pytorch cross entropy loss needs 0 and 1 not 1 and 2\nnew_labels_mapping = {}\nfor key, value in labels_mapping.items():\n    new_labels_mapping[key] = value - 1  # Subtract 1 to convert 1,2 to 0,1\n\n# Recreate your dataset with adjusted labels\ndataset = CC200Data(\"/kaggle/input/Outputs/cpac/nofilt_noglobal/rois_cc200\", cc200_to_yeo7_mapping, new_labels_mapping)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ntrain_loader = DataLoader(dataset, batch_size=16, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:08:51.735726Z","iopub.execute_input":"2025-04-18T16:08:51.735968Z","iopub.status.idle":"2025-04-18T16:08:51.741181Z","shell.execute_reply.started":"2025-04-18T16:08:51.735948Z","shell.execute_reply":"2025-04-18T16:08:51.740434Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"unique_labels = set()\nfor _, _, labels in train_loader:\n    unique_labels.update(labels.numpy())\nprint(f\"updated label values: {sorted(list(unique_labels))}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:08:51.741937Z","iopub.execute_input":"2025-04-18T16:08:51.742240Z","iopub.status.idle":"2025-04-18T16:08:51.920637Z","shell.execute_reply.started":"2025-04-18T16:08:51.742217Z","shell.execute_reply":"2025-04-18T16:08:51.919874Z"}},"outputs":[{"name":"stdout","text":"updated label values: [0, 1]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"shapes = []\na = next(iter(dataset))[1]\nfor i in a:\n    shapes.append(i.shape[0])\n\n# len(shapes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:08:51.921425Z","iopub.execute_input":"2025-04-18T16:08:51.921657Z","iopub.status.idle":"2025-04-18T16:08:51.925831Z","shell.execute_reply.started":"2025-04-18T16:08:51.921631Z","shell.execute_reply":"2025-04-18T16:08:51.925184Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MHSA(nn.Module):\n    def __init__(self, embd_dim, num_heads):\n        super().__init__()\n        self.embd_dim = embd_dim\n        self.num_heads = num_heads\n        self.head_size = self.embd_dim // self.num_heads\n        self.q = nn.Linear(self.embd_dim, self.embd_dim)\n        self.k = nn.Linear(self.embd_dim, self.embd_dim)\n        self.v = nn.Linear(self.embd_dim, self.embd_dim)\n        self.d = self.head_size ** 0.5\n        self.mlp = nn.Linear(self.embd_dim, self.embd_dim)\n        self.layer_norm = nn.LayerNorm(self.embd_dim)  \n        \n    def forward(self, x):\n        batch_size, M, _ = x.shape\n        norm = self.layer_norm(x)\n        q = self.q(norm).view(batch_size, M, self.num_heads, self.head_size).transpose(1, 2)\n        k = self.k(norm).view(batch_size, M, self.num_heads, self.head_size).transpose(1, 2)\n        v = self.v(norm).view(batch_size, M, self.num_heads, self.head_size).transpose(1, 2)\n        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / self.d\n        attn_scores = attn_scores.masked_fill(torch.eye(M, device=x.device).bool(), float('-inf'))\n        attn_weights = F.softmax(attn_scores, dim=-1)\n        context = torch.matmul(attn_weights, v).transpose(1, 2).reshape(batch_size, M, self.embd_dim)\n        out = self.mlp(context)\n        return out + x, attn_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:08:51.928401Z","iopub.execute_input":"2025-04-18T16:08:51.928721Z","iopub.status.idle":"2025-04-18T16:08:51.945221Z","shell.execute_reply.started":"2025-04-18T16:08:51.928703Z","shell.execute_reply":"2025-04-18T16:08:51.944636Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class SubnetworkEmbedder(nn.Module):\n    def __init__(self, input_dim, hidden_dim=256, output_dim=128):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim),\n        )\n\n    def forward(self, x):\n        return self.mlp(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:08:51.945934Z","iopub.execute_input":"2025-04-18T16:08:51.946151Z","iopub.status.idle":"2025-04-18T16:08:51.968860Z","shell.execute_reply.started":"2025-04-18T16:08:51.946130Z","shell.execute_reply":"2025-04-18T16:08:51.968241Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class RegionEmbedder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.region_conv = nn.Conv2d(1, 1, kernel_size=1)\n        self.mlp = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim),\n        )       \n\n    def forward(self, x):\n        x_conv = self.region_conv(x.unsqueeze(1)) \n        x_conv = x_conv.squeeze(1)  \n        return self.mlp(x_conv)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:08:51.969781Z","iopub.execute_input":"2025-04-18T16:08:51.969985Z","iopub.status.idle":"2025-04-18T16:08:51.987928Z","shell.execute_reply.started":"2025-04-18T16:08:51.969970Z","shell.execute_reply":"2025-04-18T16:08:51.987402Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class RegionEncoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, embd_dim, num_heads, num_layers):\n        super().__init__()\n        self.reg_embd = RegionEmbedder(input_dim, hidden_dim, embd_dim)\n        self.mhsa_layers = nn.ModuleList([MHSA(embd_dim, num_heads) for _ in range(num_layers)])\n\n    def forward(self, x):\n        x_reg = self.reg_embd(x)\n        x_in = x_reg\n        attn_weights_all = []\n        for mhsa in self.mhsa_layers:\n            x_in, attn_weights = mhsa(x_in)\n            attn_weights_all.append(attn_weights)\n        \n        return x_reg + x_in, torch.stack(attn_weights_all).permute(1, 0, 2, 3, 4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:08:51.988525Z","iopub.execute_input":"2025-04-18T16:08:51.988726Z","iopub.status.idle":"2025-04-18T16:08:52.012143Z","shell.execute_reply.started":"2025-04-18T16:08:51.988711Z","shell.execute_reply":"2025-04-18T16:08:52.011642Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class SubNetworkEncoder(nn.Module):\n    def __init__(self, shapes, hidden_dim, embd_dim, num_heads, num_layers):\n        super().__init__()\n        self.embd_dim = embd_dim\n        self.mlps = nn.ModuleList([SubnetworkEmbedder(i, hidden_dim, embd_dim) for i in shapes])\n        self.mhsa_layers = nn.ModuleList([MHSA(embd_dim, num_heads) for _ in range(num_layers)])\n\n    def forward(self, x):\n        batch_size = x[0].shape[0]\n        x = torch.stack([mlp(f) for mlp, f in zip(self.mlps, x)], dim=1)\n        attn_weights_all = []\n        for mhsa in self.mhsa_layers:\n            x, attn_weights = mhsa(x)\n            attn_weights_all.append(attn_weights)\n\n        return x, torch.stack(attn_weights_all).permute(1, 0, 2, 3, 4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:10:34.510683Z","iopub.execute_input":"2025-04-18T16:10:34.510985Z","iopub.status.idle":"2025-04-18T16:10:34.516950Z","shell.execute_reply.started":"2025-04-18T16:10:34.510965Z","shell.execute_reply":"2025-04-18T16:10:34.516142Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class StepOne(nn.Module):\n    def __init__(self, input_dim, hidden_dim, embd_dim, num_heads, num_layers):\n        super().__init__()\n        self.reg_enc = RegionEncoder(input_dim, hidden_dim, embd_dim, num_heads, num_layers)\n        self.subnet_enc = SubNetworkEncoder(shapes, hidden_dim, embd_dim, num_heads, num_layers)\n        self.layer_norm = nn.LayerNorm(embd_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(embd_dim, hidden_dim),  \n            nn.ReLU(),             \n            nn.Linear(hidden_dim, embd_dim)    \n        )\n        self.region_start_indices = list(dataset.region_start_indices.values()) + [185]\n\n    def subNetworkAttendRegions(self, subnet_attn_map, region_attn_map):\n        region_to_subnet = torch.zeros(185, dtype=torch.long)\n        for subnet_id in range(7):\n            start = self.region_start_indices[subnet_id]\n            end = self.region_start_indices[subnet_id + 1]\n            region_to_subnet[start:end] = subnet_id  \n        subnet_i = region_to_subnet.view(-1, 1).expand(185, 185) \n        subnet_j = region_to_subnet.view(1, -1).expand(185, 185)  \n\n        mask = subnet_i != subnet_j  \n\n        attn_multiplier = subnet_attn_map[:, :, :, subnet_i, subnet_j]  \n        attn_multiplier = attn_multiplier * mask\n\n        return region_attn_map * attn_multiplier \n    \n    def sinkhorn(self, attn, n_iters=5, eps=1e-6):\n        attn = attn + eps  \n        for _ in range(n_iters):\n            attn = attn / attn.sum(dim=-1, keepdim=True)\n            attn = attn / attn.sum(dim=-2, keepdim=True)\n        return attn\n    \n    def forward(self, x):\n        x0 = self.reg_enc(x[0])\n        x1 = self.subnet_enc(x[1])\n        o = torch.cat((x0[0], x1[0]), dim=1)\n        o_norm = self.layer_norm(o)\n        o_norm = self.mlp(o)\n        o = o + o_norm\n        print(o.shape)\n        o_reg = o[:, :185, :]\n        o_sub = o[:, 185:, :]\n        adj_matrix = self.subNetworkAttendRegions(x1[1], x0[1])\n        adj_matrix = self.sinkhorn(adj_matrix)\n        return o_reg, o_sub, adj_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:08:52.031589Z","iopub.execute_input":"2025-04-18T16:08:52.032082Z","iopub.status.idle":"2025-04-18T16:08:52.048745Z","shell.execute_reply.started":"2025-04-18T16:08:52.032065Z","shell.execute_reply":"2025-04-18T16:08:52.048150Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# HGCN part now","metadata":{}},{"cell_type":"code","source":"class HGCN(nn.Module):\n    def __init__(self, input_dim, output_dim, num_layers=4, num_heads=16):\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n\n        # One linear layer per head per layer: (L, H, in, out)\n        self.W = nn.Parameter(torch.randn(num_layers, num_heads, input_dim, output_dim))\n        self.activation = nn.ReLU()\n\n    def forward(self, features, attention_maps):\n        \"\"\"\n        features: [B, N, F_in]\n        attention_maps: [B, L, H, N, N] — soft adjacency or incidence maps\n        \"\"\"\n        B, L, H, N, _ = attention_maps.shape\n        F_in, F_out = self.input_dim, self.output_dim\n\n        # Apply W to input features: [B, 1, 1, N, F_in] x [L, H, F_in, F_out]\n        # -> output: [B, L, H, N, F_out]\n        features_exp = features[:, None, None, :, :]  # [B, 1, 1, N, F_in]\n        weights = self.W[None, :, :, :, :]            # [1, L, H, F_in, F_out]\n        transformed = torch.matmul(features_exp, weights)  # [B, L, H, N, F_out]\n\n        # Apply hypergraph attention maps\n        # attention_maps: [B, L, H, N, N]\n        # transformed:     [B, L, H, N, F_out]\n        output = torch.matmul(attention_maps, transformed)  # [B, L, H, N, F_out]\n        output = self.activation(output)\n\n        # Concatenate heads → [B, L, N, H * F_out]\n        output = output.permute(0, 1, 3, 2, 4).reshape(B, L, N, H * F_out)\n\n        # Concatenate layers → [B, N, L * H * F_out]\n        output = output.permute(0, 2, 1, 3).reshape(B, N, L * H * F_out)\n\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:08:52.049365Z","iopub.execute_input":"2025-04-18T16:08:52.049642Z","iopub.status.idle":"2025-04-18T16:08:52.070076Z","shell.execute_reply.started":"2025-04-18T16:08:52.049599Z","shell.execute_reply":"2025-04-18T16:08:52.069562Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class StepOneWithHGCN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, embd_dim, num_heads, num_layers, num_classes=2):\n        super().__init__()\n        # Copy paste\n        self.reg_enc = RegionEncoder(input_dim, hidden_dim, embd_dim, num_heads, num_layers)\n        self.subnet_enc = SubNetworkEncoder(shapes, hidden_dim, embd_dim, num_heads, num_layers)\n        self.layer_norm = nn.LayerNorm(embd_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(embd_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, embd_dim)\n        )\n        self.region_start_indices = list(dataset.region_start_indices.values()) + [185]\n        self.hgcn = HGCN(input_dim=embd_dim, output_dim=embd_dim//num_heads)\n        hgcn_output_dim = (embd_dim//num_heads) * num_heads * num_layers\n        self.classifier = nn.Sequential(\n            nn.Linear(hgcn_output_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_dim, num_classes)\n        )\n    \n    def subNetworkAttendRegions(self, subnet_attn_map, region_attn_map):\n        region_to_subnet = torch.zeros(185, dtype=torch.long)\n        for subnet_id in range(7):\n            start = self.region_start_indices[subnet_id]\n            end = self.region_start_indices[subnet_id + 1]\n            region_to_subnet[start:end] = subnet_id\n        subnet_i = region_to_subnet.view(-1, 1).expand(185, 185)\n        subnet_j = region_to_subnet.view(1, -1).expand(185, 185)\n        mask = subnet_i != subnet_j\n        attn_multiplier = subnet_attn_map[:, :, :, subnet_i, subnet_j]\n        mask = mask.to(\"cuda\")\n        attn_multiplier = attn_multiplier * mask\n        return region_attn_map * attn_multiplier\n    \n    def sinkhorn(self, attn, n_iters=5, eps=1e-6):\n        attn = attn + eps\n        for _ in range(n_iters):\n            attn = attn / attn.sum(dim=-1, keepdim=True)\n            attn = attn / attn.sum(dim=-2, keepdim=True)\n        return attn\n    \n    def forward(self, x):\n        x0, region_attn = self.reg_enc(x[0])\n        x1, subnet_attn = self.subnet_enc(x[1])\n\n        o = torch.cat((x0, x1), dim=1)\n        o_norm = self.layer_norm(o)\n        o_norm = self.mlp(o_norm)\n        o = o + o_norm\n        o_reg = o[:, :185, :]  # First 185 nodes are regions\n        o_sub = o[:, 185:, :]  # Remaining nodes are subnetworks\n        # Process attention maps to create the combined attention \n        # with shape [batch_size, num_layers (4), num_heads (16), 185, 185]\n        combined_attn = self.subNetworkAttendRegions(subnet_attn, region_attn)\n        combined_attn = self.sinkhorn(combined_attn)\n        \n        # Pass through HGCN - only process the region features with the combined attention\n        hgcn_output = self.hgcn(o_reg, combined_attn)\n        # Global average pooling for classification\n        pooled_output = hgcn_output.mean(dim=1)  # [batch_size, output_dim*num_heads*num_layers]\n        # Final classif\n        logits = self.classifier(pooled_output)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:11:12.510877Z","iopub.execute_input":"2025-04-18T16:11:12.511426Z","iopub.status.idle":"2025-04-18T16:11:12.521726Z","shell.execute_reply.started":"2025-04-18T16:11:12.511401Z","shell.execute_reply":"2025-04-18T16:11:12.520973Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from tqdm import tqdm\n\nmodel = StepOneWithHGCN(\n    input_dim=185,\n    hidden_dim=256,\n    embd_dim=128,\n    num_heads=16,\n    num_layers=4,\n    num_classes=2\n)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\ncriterion = nn.CrossEntropyLoss()\n\nnum_epochs = 50\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    # tqdm for batch progress\n    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n    \n    for batch_idx, (region_data, subnetwork_data, labels) in pbar:\n        region_data = region_data.to(device)\n        subnetwork_data = [subnet.to(device) for subnet in subnetwork_data]\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n\n        x = (region_data, subnetwork_data)\n        logits = model(x)\n        loss = criterion(logits, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(logits.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n        pbar.set_postfix({\n            'Loss': f'{running_loss / (batch_idx + 1):.4f}',\n            'Acc': f'{100 * correct / total:.2f}%'\n        })\n\ntorch.save(model.state_dict(), 'brain_network_model.pth')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:16:06.523852Z","iopub.execute_input":"2025-04-18T16:16:06.524172Z","iopub.status.idle":"2025-04-18T16:24:43.591253Z","shell.execute_reply.started":"2025-04-18T16:16:06.524151Z","shell.execute_reply":"2025-04-18T16:24:43.590665Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/50: 100%|██████████| 56/56 [00:10<00:00,  5.41it/s, Loss=0.8238, Acc=47.62%]\nEpoch 2/50: 100%|██████████| 56/56 [00:10<00:00,  5.39it/s, Loss=0.7756, Acc=49.43%]\nEpoch 3/50: 100%|██████████| 56/56 [00:10<00:00,  5.38it/s, Loss=0.7662, Acc=49.10%]\nEpoch 4/50: 100%|██████████| 56/56 [00:10<00:00,  5.42it/s, Loss=0.7298, Acc=52.71%]\nEpoch 5/50: 100%|██████████| 56/56 [00:10<00:00,  5.44it/s, Loss=0.7462, Acc=49.77%]\nEpoch 6/50: 100%|██████████| 56/56 [00:10<00:00,  5.45it/s, Loss=0.7277, Acc=49.66%]\nEpoch 7/50: 100%|██████████| 56/56 [00:10<00:00,  5.47it/s, Loss=0.7123, Acc=50.23%]\nEpoch 8/50: 100%|██████████| 56/56 [00:10<00:00,  5.46it/s, Loss=0.7065, Acc=48.98%]\nEpoch 9/50: 100%|██████████| 56/56 [00:10<00:00,  5.45it/s, Loss=0.7023, Acc=51.70%]\nEpoch 10/50: 100%|██████████| 56/56 [00:10<00:00,  5.42it/s, Loss=0.7073, Acc=50.68%]\nEpoch 11/50: 100%|██████████| 56/56 [00:10<00:00,  5.42it/s, Loss=0.6971, Acc=51.92%]\nEpoch 12/50: 100%|██████████| 56/56 [00:10<00:00,  5.41it/s, Loss=0.7050, Acc=51.70%]\nEpoch 13/50: 100%|██████████| 56/56 [00:10<00:00,  5.40it/s, Loss=0.6985, Acc=50.23%]\nEpoch 14/50: 100%|██████████| 56/56 [00:10<00:00,  5.39it/s, Loss=0.6992, Acc=53.17%]\nEpoch 15/50: 100%|██████████| 56/56 [00:10<00:00,  5.38it/s, Loss=0.6991, Acc=50.90%]\nEpoch 16/50: 100%|██████████| 56/56 [00:10<00:00,  5.39it/s, Loss=0.7006, Acc=50.90%]\nEpoch 17/50: 100%|██████████| 56/56 [00:10<00:00,  5.40it/s, Loss=0.6996, Acc=53.73%]\nEpoch 18/50: 100%|██████████| 56/56 [00:10<00:00,  5.41it/s, Loss=0.6933, Acc=53.96%]\nEpoch 19/50: 100%|██████████| 56/56 [00:10<00:00,  5.41it/s, Loss=0.6980, Acc=53.05%]\nEpoch 20/50: 100%|██████████| 56/56 [00:10<00:00,  5.40it/s, Loss=0.6952, Acc=51.36%]\nEpoch 21/50: 100%|██████████| 56/56 [00:10<00:00,  5.41it/s, Loss=0.6897, Acc=54.75%]\nEpoch 22/50: 100%|██████████| 56/56 [00:10<00:00,  5.40it/s, Loss=0.6949, Acc=49.77%]\nEpoch 23/50: 100%|██████████| 56/56 [00:10<00:00,  5.40it/s, Loss=0.6950, Acc=53.17%]\nEpoch 24/50: 100%|██████████| 56/56 [00:10<00:00,  5.41it/s, Loss=0.6920, Acc=54.19%]\nEpoch 25/50: 100%|██████████| 56/56 [00:10<00:00,  5.41it/s, Loss=0.6913, Acc=54.19%]\nEpoch 26/50: 100%|██████████| 56/56 [00:10<00:00,  5.42it/s, Loss=0.6906, Acc=54.19%]\nEpoch 27/50: 100%|██████████| 56/56 [00:10<00:00,  5.42it/s, Loss=0.6919, Acc=53.05%]\nEpoch 28/50: 100%|██████████| 56/56 [00:10<00:00,  5.41it/s, Loss=0.6912, Acc=54.30%]\nEpoch 29/50: 100%|██████████| 56/56 [00:10<00:00,  5.41it/s, Loss=0.6921, Acc=53.17%]\nEpoch 30/50: 100%|██████████| 56/56 [00:10<00:00,  5.41it/s, Loss=0.6915, Acc=54.07%]\nEpoch 31/50: 100%|██████████| 56/56 [00:10<00:00,  5.42it/s, Loss=0.6899, Acc=53.85%]\nEpoch 32/50: 100%|██████████| 56/56 [00:10<00:00,  5.42it/s, Loss=0.6911, Acc=53.62%]\nEpoch 33/50: 100%|██████████| 56/56 [00:10<00:00,  5.42it/s, Loss=0.6929, Acc=53.73%]\nEpoch 34/50: 100%|██████████| 56/56 [00:10<00:00,  5.42it/s, Loss=0.6905, Acc=53.96%]\nEpoch 35/50: 100%|██████████| 56/56 [00:10<00:00,  5.42it/s, Loss=0.6904, Acc=54.41%]\nEpoch 36/50: 100%|██████████| 56/56 [00:10<00:00,  5.43it/s, Loss=0.6885, Acc=54.41%]\nEpoch 37/50: 100%|██████████| 56/56 [00:10<00:00,  5.42it/s, Loss=0.6914, Acc=53.05%]\nEpoch 38/50: 100%|██████████| 56/56 [00:10<00:00,  5.43it/s, Loss=0.6903, Acc=53.39%]\nEpoch 39/50: 100%|██████████| 56/56 [00:10<00:00,  5.43it/s, Loss=0.6910, Acc=53.62%]\nEpoch 40/50: 100%|██████████| 56/56 [00:10<00:00,  5.42it/s, Loss=0.6917, Acc=53.62%]\nEpoch 41/50: 100%|██████████| 56/56 [00:10<00:00,  5.42it/s, Loss=0.6910, Acc=53.28%]\nEpoch 42/50: 100%|██████████| 56/56 [00:10<00:00,  5.43it/s, Loss=0.6920, Acc=53.39%]\nEpoch 43/50: 100%|██████████| 56/56 [00:10<00:00,  5.43it/s, Loss=0.6912, Acc=54.75%]\nEpoch 44/50: 100%|██████████| 56/56 [00:10<00:00,  5.43it/s, Loss=0.6915, Acc=53.05%]\nEpoch 45/50: 100%|██████████| 56/56 [00:10<00:00,  5.41it/s, Loss=0.6990, Acc=51.58%]\nEpoch 46/50: 100%|██████████| 56/56 [00:10<00:00,  5.41it/s, Loss=0.7063, Acc=50.23%]\nEpoch 47/50: 100%|██████████| 56/56 [00:10<00:00,  5.42it/s, Loss=0.7278, Acc=47.74%]\nEpoch 48/50: 100%|██████████| 56/56 [00:10<00:00,  5.42it/s, Loss=0.7281, Acc=50.34%]\nEpoch 49/50: 100%|██████████| 56/56 [00:10<00:00,  5.43it/s, Loss=0.6947, Acc=53.51%]\nEpoch 50/50: 100%|██████████| 56/56 [00:10<00:00,  5.43it/s, Loss=0.6895, Acc=53.39%]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}